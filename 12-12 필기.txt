■ 162 ~ 172. 웹스크롤링

	1. 웹 스크롤링 기본 문법
	2. ebs 레이디 버그 게시판 댓글 스크롤링
	3. 한겨례 신문사 웹 스크롤링 코드 구현
	4. 중앙일보 신문사
	5. 동아일보 신문사

		주가에 영향을 주는 요소 빅데이터
			기사를 읽는 인공지능 신경망(아직 없음)
			( 미래에셋 로보 어드바이져 )
 
	6. 이미지(사진) 데이터 스크롤링
		- 네이버
		- 구글
		- 빙
		- 다음



문제 389. 중앙일보 신문사 사이트에서 인공지능으로 검색한 url을 알아내시오 !

	https://search.joins.com/TotalNews?page=1&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A #
	%A5&SortType=New&SearchCategoryType=TotalNews
	
	https://search.joins.com/TotalNews?page=2&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A #
	%A5&SortType=New&SearchCategoryType=TotalNews


-------------------------------------------------------------------------------------
- 어제 만들었던 한겨례 사이트 웹스크롤 함수 2개 가져온다
	1. hani_scroll() : 상세기사 url 가져와서 params 리스트에 담는 함수
	2. hani_scroll() : 상세기사 url로 상세기사 본문을 스크롤링하는 함수


	import re
	import urllib.request
	from bs4 import BeautifulSoup
	
	def chuan_scroll():
	    params = []
	    f = open('c:\\mytext9.txt', 'w', encoding = 'UTF-8')
	    for i in range(1, 5):
	        list_url = "https://search.joins.com/TotalNews?page="+ str(i) + "&Keyword=%EC%9D%B8%EA%B3%B5 \
			    %EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=TotalNews"
	        url = urllib.request.Request(list_url)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for j in soup.find_all('strong', class_='headline mg'):
	            for i2 in j:
	                params.append(i2.get('href'))
	    params2 = []
	    for i in params:
	        url = urllib.request.Request(i)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('div', id="article_body"): 	#찾는 태그로 id를 이용
	            params2.append(i.get_text())
	    for item in params2:
	        f.write('%s\n' %item)
	    print('저장되었습니다.')
	            
	            
	chuan_scroll()
	
	
	※ 설명 : html 코드에서 id란?
		잘 설계된 html코드 문서라면 id의 값은 html문서 내에 단 한개만 존재한다.
-----------------------------------------------------------------------------------------


문제 390. 위에서 긁어온 데이터를 mydata9.txt로 생성하시오 !

	import re
	import urllib.request
	from bs4 import BeautifulSoup
	
	def chuan_scroll():
	    params = []
	    f = open('c:\\mytext9.txt', 'w', encoding = 'UTF-8')
	    for i in range(1, 5):
	        list_url = "https://search.joins.com/TotalNews?page="+ str(i) + "&Keyword=%EC%9D%B8%EA%B3%B5 \
			    %EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=TotalNews"
	        url = urllib.request.Request(list_url)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for j in soup.find_all('strong', class_='headline mg'):
	            for i2 in j:
	                params.append(i2.get('href'))
	    params2 = []
	    for i in params:
	        url = urllib.request.Request(i)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('div', id="article_body"): 
	            params2.append(i.get_text())
	    for item in params2:
	        f.write('%s\n' %item)
	    print('저장되었습니다.')
	            
	            
	chuan_scroll()



문제 391. 동아일보 신문사에서 딥러닝으로 검색한 기사를 mytext10.txt에 담는 함수 2개를 생성하시오 !

	import urllib.request
	from bs4 import BeautifulSoup
	
	def donga_scroll():
	    params = []
	    f = open('c:\\mytext10.txt', 'w', encoding = 'UTF-8')
	    for i in range(1, 100, 15):
	        list_url = "http://news.donga.com/search?p="+ str(i) + "&query=%EB%94%A5%EB%9F%AC%EB%8B	%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1"
	        url = urllib.request.Request(list_url)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('p', class_='tit'):
	            result = i.find('a').get('href')
	            params.append(result)
	    params2 = []
	    for i in params:
	        url = urllib.request.Request(i)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('div',class_="article_txt"): 
	            for i2 in i(["script"]):
	                i2.decompose()
	            params2.append(i.get_text())
	    for item in params2:
	        f.write('%s\n' %item)
	    print('저장되었습니다.')
	
	donga_scroll()



문제 392. 위의 문제를 워드클라우드로 출력하시오 !

	import urllib.request
	from bs4 import BeautifulSoup
	
	def donga_scroll():
	    params = []
	    f = open('c:\\mytext10.txt', 'w', encoding = 'UTF-8')
	    for i in range(1, 100, 15):
	        list_url = "http://news.donga.com/search?p="+ str(i) + "&query=%EB%94%A5%EB%9F%AC%EB%8B	%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1"
	        url = urllib.request.Request(list_url)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('p', class_='tit'):
	            result = i.find('a').get('href')
	            params.append(result)
	    params2 = []
	    for i in params:
	        url = urllib.request.Request(i)
	        result = urllib.request.urlopen(url).read().decode("utf-8")
	        soup = BeautifulSoup( result , "html.parser")
	        for i in soup.find_all('div',class_="article_txt"): 
	            for i2 in i(["script"]):
	                i2.decompose()
	            params2.append(i.get_text())
	    for item in params2:
	        f.write('%s\n' %item)
	    print('저장되었습니다.')
	
	donga_scroll()
	
	import Text_mining
	tm = Text_mining.Text_mining()
	tm.word_cloud()



문제 393. (이미지 스크롤링) 네이버에서 아이언맨으로 이미지 검색을 했을 때 나오는 이미지들을 모두 
			    스크롤링하시오 !

	* 이미지 스크롤링 하려면 필요한 내용 2가지

		1. selenium 설치		# 사람이 손으로 스크롤링하는것을 파이썬으로 구현하는 것
			* 아나콘다 프롬프트 창 열고
				(base) C:\Users\Administrator>conda install selenium

		2. 크롬 드라이버 ( 크롬으로 스크롤링 )
			c:\\chromedriver <---- 폴더 생성후에 여기에 chromdriver.exe를 넣으시오

		3. c드라이브 밑에 naver_Images 라는 폴더를 생성하시오 !

	import  urllib.request
	from  bs4  import  BeautifulSoup
	from selenium import webdriver
	from selenium.webdriver.common.keys import Keys
	import time   #중간중간에 sleep을 넣기 위해
	
	binary = 'c:\chromedriver/chromedriver.exe'   #크롬 여는 장치
	browser = webdriver.Chrome(binary)
	browser.get("https://search.naver.com/search.naver?where=image&amp;sm=stb_nmr&amp;")   #이미지 검색 url
	elem = browser.find_element_by_id("nx_query")   # 검색창의 id
	#find_elements_by_class_name("")
	
	# 검색어 입력
	elem.send_keys("조로")
	elem.submit()
	
	# 반복할 횟수
	for i in range(1,20):
	    browser.find_element_by_xpath("//body").send_keys(Keys.END)   #바디를 누르고 END키를 눌러 아래로 내려감
	    time.sleep(5)   #5초 슬립 이유 : 인터넷 환경에 따라 빨리 이미지 안나옴

	time.sleep(5)   #마지막 스크롤바 내리고 5초 쉬고
	html = browser.page_source
	soup = BeautifulSoup(html,"html.parser")
	print(soup)
	print(len(soup))
	
	def fetch_list_url():
	    params = []
	    imgList = soup.find_all("img", class_="_img")
	    for im in imgList:
	        params.append(im["src"])
	    return params
	
	def  fetch_detail_url():
	    params = fetch_list_url()
	    #print(params)
	    a = 1
	    for p in params:
	        # 다운받을 폴더경로 입력
	        urllib.request.urlretrieve(p, "c:/naver_Images/"+ str(a) + ".jpg" )
	        a += 1
	
	fetch_detail_url()
	
	browser.quit()




문제 394. (이미지 스크롤링) 구글에서 이미지 검색하는 웹 스크롤링 코드를 작성하시오 !

	import  urllib.request
	from  bs4  import  BeautifulSoup
	from selenium import webdriver
	from selenium.webdriver.common.keys import Keys
	import time
	
	binary = 'c:\chromedriver/chromedriver.exe'
	browser = webdriver.Chrome(binary)
	browser.get("https://www.google.co.kr/imghp?hl=ko")
	elem = browser.find_element_by_class_name("gLFyf")
	#find_elements_by_class_name("")
	
	# 검색어 입력
	elem.send_keys("조로")
	elem.submit()
	
	# 반복할 횟수
	for i in range(1 ,10):
	    browser.find_element_by_xpath("//body").send_keys(Keys.END)
	    try:
	        browser.find_element_by_id("smb").click()
	        time.sleep(5)
	    except:
	        time.sleep(5)
	
	time.sleep(10)
	html = browser.page_source
	soup = BeautifulSoup(html,"html.parser")
	#print(soup)
	#print(len(soup))
	
	def fetch_list_url():
	    params = []
	    imgList = soup.find_all("img", class_="rg_ic rg_i")
	    for im in imgList:
	        try:
	            params.append(im["src"])
	        except KeyError:
	            params.append(im["data-src"])
	    return params
	
	
	def  fetch_detail_url():
	    params = fetch_list_url()
	    #print(params)
	    a = 1
	    for p in params:
	        # 다운받을 폴더경로 입력
	        urllib.request.urlretrieve(p, "c:/naver_Images/"+ str(a) + ".jpg" )
	        a += 1
	fetch_detail_url()
	
	browser.quit()



문제 395. (이미지 스크롤링) 다음 이미지 검색에서 햄버거로 검색했을 때 나오는 이미지들을 c드라이브 밑에 
			     daumImages라는 폴더에 저장되게 하시오 !

	import  urllib.request
	from  bs4  import  BeautifulSoup
	from selenium import webdriver
	from selenium.webdriver.common.keys import Keys
	import time
	
	binary = 'c:\chromedriver/chromedriver.exe'
	browser = webdriver.Chrome(binary)
	browser.get("https://search.daum.net/search?w=img&nil_search=btn&DA=NTB&enc=utf8&q=")
	elem = browser.find_element_by_id("q")
	#find_elements_by_class_name("")
	
	# 검색어 입력
	elem.send_keys("조로")
	elem.submit()
	
	# 반복할 횟수
	for i in range(1,3):
	    browser.find_element_by_xpath("//body").send_keys(Keys.END)
	    time.sleep(5)
	
	time.sleep(5)
	html = browser.page_source
	soup = BeautifulSoup(html,"html.parser")
	#print(soup)
	#print(len(soup))
	
	def fetch_list_url():
	    params = []
	    imgList = soup.find_all("img", class_="thumb_img")
	    for im in imgList:
	        params.append(im["src"])
	    return params
	
	
	def  fetch_detail_url():
	    params = fetch_list_url()
	    #print(params)
	    a = 1
	    for p in params:
	        # 다운받을 폴더경로 입력
	        urllib.request.urlretrieve(p, "c:/daum_Images/"+ str(a) + ".jpg" )
	        a += 1
	fetch_detail_url()
	
	browser.quit()



문제 395. (이미지 스크롤링) Bing 에서 이미지 검색을 조회했을 때 나오는 이미지들을 c 드라이브 밑에 bing이라는
			    폴더에 저장하시오 !

	import  urllib.request
	from  bs4  import  BeautifulSoup
	from selenium import webdriver
	from selenium.webdriver.common.keys import Keys
	import time
	
	binary = 'c:\chromedriver/chromedriver.exe'
	browser = webdriver.Chrome(binary)
	browser.get("https://www.bing.com/?scope=images&FORM=Z9LH1")
	elem = browser.find_element_by_id("sb_form_q")
	#find_elements_by_class_name("")
	
	# 검색어 입력
	elem.send_keys("조로")
	elem.submit()
	
	# 반복할 횟수
	for i in range(1 ,10):
	    browser.find_element_by_xpath("//body").send_keys(Keys.END)
	    time.sleep(5)
	
	
	time.sleep(5)
	html = browser.page_source
	soup = BeautifulSoup(html,"html.parser")
	#print(soup)
	#print(len(soup))
	
	def fetch_list_url():
	    params = []
	    imgList = soup.find_all("img", class_="mimg")
	    for im in imgList:
	        params.append(im["src"])
	    return params
	
	def  fetch_detail_url():
	    params = fetch_list_url()
	    #print(params)
	    a = 1
	    for p in params:
	        # 다운받을 폴더경로 입력
	        urllib.request.urlretrieve(p, "c:/Bing_Images/"+ str(a) + ".jpg" )
	        a += 1
	fetch_detail_url()
	
	browser.quit()


문제 396. (오늘의 마지막 문제) instagram에서 이미지를 다운로드 받는 웹 스크롤링 코드를 작성하시오 !

	import  urllib.request
	from  bs4  import  BeautifulSoup
	from selenium import webdriver
	from selenium.webdriver.common.keys import Keys
	import time
	
	binary = 'c:\chromedriver/chromedriver.exe'
	browser = webdriver.Chrome(binary)
	browser.get("https://www.instagram.com/instagram/")
	browser.find_element_by_class_name("TqC_a").click()
	browser.find_element_by_css_selector("input.XTCLo.x3qfX").clear()
	browser.find_element_by_css_selector("input.XTCLo.x3qfX").send_keys('#에일리')
	time.sleep(2)
	browser.find_element_by_css_selector("input.XTCLo.x3qfX").send_keys(Keys.ENTER)
	browser.find_element_by_css_selector("input.XTCLo.x3qfX").send_keys(Keys.ENTER)
	time.sleep(4)
	# 반복할 횟수
	for i in range(1 ,4):
	    browser.find_element_by_xpath("//body").send_keys(Keys.END)
	    time.sleep(5)
	
	
	time.sleep(5)
	html = browser.page_source
	soup = BeautifulSoup(html,"html.parser")
	#print(soup)
	#print(len(soup))
	
	def fetch_list_url():
	    params = []
	    imgList = soup.find_all("img", class_="FFVAD")
	    for im in imgList:
	        params.append(im["src"])
	    return params
	
	def  fetch_detail_url():
	    params = fetch_list_url()
	    #print(params)
	    a = 1
	    for p in params:
	        # 다운받을 폴더경로 입력
	        urllib.request.urlretrieve(p, "c:/instagram_Images/"+ str(a) + ".jpg" )
	        a += 1
	fetch_detail_url()
	
	browser.quit()


		※ 참고
		html개발 지원 창 콘솔창 활용하기
		 	- document.querySelectorAll(" ")
			- 상위 엘리먼트는 띄어쓰기
			- 클래스는 .
				- 엘리먼트.클래스이름
			- 아이디는 #
				- 엘리먼트#아이디이름

			* 콘솔 언어 파이썬에서 활용
				- find_element_by_css_selector(" ")
























